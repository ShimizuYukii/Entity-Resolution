---
title: "Fastlink"
author: "Haocheng Qin, Kewei Xu, Zaolin Zhang, Chuhan Guo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Methodology

Literature: https://imai.fas.harvard.edu/research/files/linkage.pdf

To conclude the methodology section of the paper, the authors developed a fast and scalable probabilistic model designed to merge large-scale administrative records more effectively than traditional deterministic methods. This model, called fastLink, addresses issues such as missing data, measurement error, and the uncertainty inherent in merging processes, which are common in social science research. By incorporating auxiliary information, such as name frequency or migration rates, and allowing for robust simulation studies, the proposed methodology significantly outperforms deterministic approaches. Furthermore, it offers an open-source solution to merge data sets efficiently while providing tools for post-merge analyses that account for the uncertainty of the matching process​

## Analysis

The advantage of probabilistic models, like the one proposed, is their ability to quantify the inherent uncertainty in the merging process. After estimating the model parameters, match probabilities for each pair of records can be computed using Bayes' rule. This allows researchers to calculate match probabilities for each pair and account for uncertainty in downstream analysis. Researchers can set a threshold probability to determine whether pairs are matched, with trade-offs in terms of false positives and negatives. Probabilistic models allow for the estimation of false discovery rates (FDR) and false negative rates (FNR), giving insight into the quality of the matches and enabling post-merge analyses that consider uncertainty​(linkage).

Uncertainty Analysis

The methodology is designed to handle large-scale datasets. The authors developed a fast, scalable algorithm that incorporates blocking and parallelization techniques to manage millions of observations. The implementation of this methodology, in the form of the R package fastLink, demonstrates significant computational efficiency. Compared to other existing open-source packages, fastLink performs faster, scales better, and can process datasets with millions of records, such as voter files and political contribution databases​(linkage). This makes it feasible for social scientists to perform record linkage in large administrative datasets without sacrificing performance or accuracy

Scalability

The authors validate their methodology through two empirical applications. The first merges election survey data from the Cooperative Congressional Election Study (CCES) with political contribution data (DIME). This merge involves over 50,000 survey responses and more than five million donor records, where the key challenge is the small expected number of matches. The second application merges two nationwide voter files, each containing over 160 million records. These applications demonstrate the scalability of the method and its ability to handle complex data merges involving millions of records. The authors also show that incorporating auxiliary data, such as migration rates, further improves matching quality

Empirical Validation

The authors validate their proposed methodology through two empirical applications:

1. Merging Election Survey Data with Political Contribution Data: 

The first validation involved merging the 2012 Cooperative Congressional Election Study (CCES) survey data with the Database on Ideology, Money in Politics, and Elections (DIME). The dataset consisted of over 54,000 CCES respondents and more than five million donor records from DIME. The challenge was the relatively small expected number of matches between these two datasets, as not all survey respondents were expected to have made political donations. The authors utilized blocking and probabilistic record linkage to complete the merge, and the results were compared to a proprietary method previously used by other researchers. They found that fastLink performed at least as well as the proprietary method and provided a more rigorous approach by incorporating post-merge uncertainty analysis​

2. Merging Two Nationwide Voter Files Over Time: 

The second application involved merging two nationwide voter files, each containing over 160 million records, from 2014 and 2015. This is possibly the largest data merge ever conducted in social sciences. The key challenge in this case was dealing with voters who changed residences, making address information unreliable for matching. The authors incorporated migration rates using auxiliary data from the IRS to improve matching accuracy. Their two-step procedure combined blocking, parallelization, and probabilistic record linkage. The results showed high match rates and low false discovery rates (FDR) and false negative rates (FNR), demonstrating the efficiency and scalability of the methodology​

## Model Framework and Structures

### Setup
The method involves merging two datasets, \( A \) and \( B \), each containing \( NA \) and \( NB \) records respectively. They use \( K \) linkage variables for comparisons. The model defines an agreement vector \( g(i, j) \) for each record pair \( (i, j) \), where \( g_k(i, j) \) defines the similarity of the k-th variable between records \( i \) from \( A \) and \( j \) from \( B \).

### Model Formulation
- **Linkage Variables**: Uses Bernoulli random variables \( M_{ij} \) that identify whether a record pair \( (i, j) \) matches (\( M_{ij} = 1 \)) or not (\( M_{ij} = 0 \)). In other words, The model uses simple yes/no variables, represented mathematically as Bernoulli random variables \( M_{ij} \). These variables help decide whether a pair of records \( (i, j) \) from two different datasets is a match (\( M_{ij} = 1 \)) or not (\( M_{ij} = 0 \)). Think of it as a sophisticated way of saying “these two records are talking about the same thing/person.”
- **Conditional Distributions**: Assumes conditional independence among linkage variables given the match status \( M_{ij} \). In other words, each variable's match status (like name, address) does not depend on each other after knowing whether the overall records match. This could enable the decomposition of the joint probability distribution into simpler, individual probabilities.
- **Handling Missing Data**: Utilizes a Missing At Random (MAR) framework to allow the omission of missing data in the probability calculations, which simplifies the likelihood function and enhances computational efficiency.

## Algorithm and Computation

### EM Algorithm
The parameter estimation is executed using the Expectation-Maximization (EM) algorithm. It starts with an initial guess, then repeatedly adjusts this guess aiming to improve the likelihood that the observed data came from the proposed model. This optimizes the observed-data likelihood function, which integrates over the probabilistic distributions of the linkage variables conditioned on the match hypotheses.

### Blocking and Filtering
To reduce computational demands:
- **Blocking**: To avoid comparing every record in one dataset with every record in another, which can be overwhelmingly time-consuming with large datasets, the model groups records into blocks based on shared characteristics (like all people with the same birth year), which greatly cuts down on unnecessary comparisons.
- **Filtering**: Eliminates highly unlikely pairs from consideration early in the process, using thresholds based on calculated probabilities.

### Scalability
The algorithm is designed to work efficiently even with very large datasets that contain millions of records. It uses parallel processing (splitting the work across multiple computer processors) and smart data structures to manage this, making it practical to run on a typical laptop without needing supercomputer resources.

## Evaluation and Implementation

### Simulation Studies
The model's robustness is tested through simulations that mimic real-world problems like incomplete data or errors in the data (measurement errors). These simulations help verify that the model can handle different types of common data issues effectively. The model is compared to traditional methods (like exact match), showing that it can handle complex, imperfect data more effectively and efficiently.

### Package Realization

Repository: https://github.com/kosukeimai/fastLink

Example: https://imai.fas.harvard.edu/research/files/turnout.pdf

### Implementation 

Dataset: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/2NNA4L

## Statistical Analysis Post-Merging

### Uncertainty Quantification
The model quantifies the uncertainty in the merging process, allowing researchers to account for potential errors in subsequent analyses, which is critical for maintaining the integrity of research conclusions.

### Post-Merge Analysis
Discusses methodologies for incorporating the probabilities of matches into regression analyses and other statistical procedures to adjust for the uncertainty inherent in the linkage process.

## Contributions and Innovations

The model makes substantial contributions to the field of data management by providing:
- A robust probabilistic framework that substantially outperforms traditional deterministic methods. Unlike older methods that just said 'yes' or 'no' to whether records match, this model calculates how likely it is that records match. This approach gives us a clearer picture and usually results in better performance.
- Enhanced handling of missing data and the independence assumptions of linkage variables, which have been a significant limitation in earlier models.
- Detailed documentation and an accessible implementation in R, which facilitates reproducible research and widespread adoption in the social sciences.

```{r}
suppressMessages(require("fastLink"))
suppressMessages(require("plyr"))
data <- read.delim("cces2016voterval.tab")
summary(data)
```

