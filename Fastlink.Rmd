---
title: "Fastlink"
author: "Haocheng Qin, Kewei Xu, Zaolin Zhang, Chuhan Guo"
output:
  pdf_document: default
  html_document: default
bibliography: references.bib
link-citations: true
csl: ieee.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

FastLink is a entity resolution methodology designed to merge large-scale datasets efficiently, addressing challenges such as missing data, measurement errors, and
uncertainty in the merging process. Compared to traditional deterministic techniques, which rely on exact matching criteria, FastLink offers more flexibility by leveraging 
auxiliary information (e.g., name frequency, migration rates) and providing a probabilistic match score. This allows for more accurate linking of records even when data is 
incomplete or imprecise. Its scalability makes it suitable for handling millions of records, outperforming many existing methods in both speed and accuracy. However, 
FastLink has limitations when dealing with long strings, such as full names or addresses, where variations and typographical errors can reduce its effectiveness, especially
without robust string-matching algorithms to support such cases.

## Methodology

Let's first describe the canonical probabilistic model of record linkage to demonstrate its properties. Materials are from the 

original papaer: https://imai.fas.harvard.edu/research/files/linkage-app.pdf 

and its online supplementary: https://imai.fas.harvard.edu/research/files/linkage-app.pdf

Let a latent mixing variable $M_{ij}$ to indicate whether a pair of
records (for the ith record in the data set A and the jth
record in the data set B). Notate $\gamma(i,j)$ to be the distance between a pair, just as Fellegi and Sunter did[@fellegi1969], it's changebable but we just use Jaro-Winkler string distance here, which is commonly used[@jaro1989]; and $\delta(i,j)$ to the missing indicator, to deal with missing data, then generally, we are estimating $\xi_{i j} :=\operatorname{Pr}\left(M_{i j}=1 \mid \delta(i, j), \gamma(i, j)\right)$. We can notice that, actually, without the missing indicator, Fastlink would be quite similar to Fellegi-Sunter. Even the assumptions are the same:

$$
\begin{aligned}
& \gamma_k(i, j) \mid M_{i j}= m \stackrel{\text { indep. }}{\sim} \operatorname{Discrete}\left(\pi_{k m}\right); \\
& M_{i j} \stackrel{\text { i.i.d. }}{\sim} \operatorname{Bernoulli}(\lambda)
\end{aligned}
$$

But this can be actually relaxed, we will discuss it later. So addressing missing data would surelly be a significant property of Fastlink. Theoretically, it relies on a missing at random assumption (i.e. $\delta_k(i, j) \perp \gamma_k(i, j) \mid M_{i j}$), but it still works well under some other conditions. With some Bayes calculation (still similar to what Fellegi-Sunter did), actually we can obtain the exact probability to be:

$$
\begin{aligned}
\xi_{i j}  =\frac{\lambda \prod_{k=1}^K\left(\prod_{\ell=0}^{L_k-1} \pi_{k \ell \ell}^{1\left\{\gamma_k(i, j)=\ell\right\}}\right)^{1-\delta_k(i, j)}}{\sum_{m=0}^1 \lambda^m(1-\lambda)^{1-m} \prod_{k=1}^K\left(\prod_{\ell=0}^{L_k-1} \pi_{k m \ell}^{1\left\{\gamma_{\ell}(i, j)=\ell\right\}}\right)^{1-\delta_k(i, j)}}
\end{aligned}
$$

For the package, this is done by getPosterior(). This introduces us two main advantages: 

Firstly, probabilistic models can quantify the uncertainty inherent in many merge procedures, offering a principled way to calibrate and account for false positives and false negatives. Also for post-merge analysis, such probability works as a good weight for merged variable, i.e. $X^*_i=\sum_{j=1}^{N_{\mathcal{B}}} \xi_{i j} X_j / \sum_{j=1}^{N_{\mathcal{B}}} \xi_{i j}$

Secondly, this proviade an easy way to compute. Intuitively, we can plug in the maximum likelihood estimation of $\lambda$ and $\pi$ here, which is

$$
L_{c o m}(\lambda, \boldsymbol{\pi} \mid \boldsymbol{\gamma}, \boldsymbol{\delta}) \propto \prod_{i=1}^{N_{A}} \prod_{j=1}^{N_{B}} \prod_{m=0}^1\left\{\lambda^m(1-\lambda)^{1-m} \prod_{k=1}^K\left(\prod_{\ell=0}^{L_k-1} \pi_{k m \ell}^{\mathbf{1}\left\{\gamma_k(i, j)=\ell\right\}}\right)^{1-\delta_k(i, j)}\right\}^{\mathbf{1}\left\{M_{i j}=m\right\}}
$$

which is hard to compute, but iteratively, we can apply EM (Expectation-Maximization) method[@winkler2000] with 
$$
\begin{aligned}
\lambda & =\frac{1}{N_{A} N_{B}} \sum_{i=1}^{N_{A}} \sum_{j=1}^{N_{B}} \xi_{i j} \\
\pi_{k m \ell} & =\frac{\left.\sum_{i=1}^{N_{A}} \sum_{j=1}^{N_{B}} \mathbf{1}\left\{\gamma_k(i, j)=l\right)\right\}\left(1-\delta_k(i, j)\right) \xi_{i j}^m\left(1-\xi_{i j}\right)^{1-m}}{\sum_{i=1}^{N_{A}} \sum_{j=1}^{N_{B}}\left(1-\delta_k(i, j)\right) \xi_{i j}^m\left(1-\xi_{i j}\right)^{1-m}}
\end{aligned}
$$

together with the calculated $\xi_{ij}$ above. You can find the codes for EM calculations in "emlinkMARmov.R", by codes you can find this package actually allows a prior for the hyperparameters, which is not shown above. 

Actually we can find another function called "emlinklog.R" in the package, this is actually accommodating a more general pattern of interaction. This algorithm allows for the inclusion of interaction terms, that is what I said the assumptions can be relaxed, but without a prior, you can choose it by state "cond.indep = False" in the main function fastlink() according to your requirements, but we will not go through the details of this algorithm here.

## Model Framework and Structures

### Setup
The method involves merging two datasets, \( A \) and \( B \), each containing \( NA \) and \( NB \) records respectively. They use \( K \) linkage variables for comparisons. The model defines an agreement vector \( g(i, j) \) for each record pair \( (i, j) \), where \( g_k(i, j) \) defines the similarity of the k-th variable between records \( i \) from \( A \) and \( j \) from \( B \).

### Model Formulation
- **Linkage Variables**: Uses Bernoulli random variables \( M_{ij} \) that identify whether a record pair \( (i, j) \) matches (\( M_{ij} = 1 \)) or not (\( M_{ij} = 0 \)). In other words, The model uses simple yes/no variables, represented mathematically as Bernoulli random variables \( M_{ij} \). These variables help decide whether a pair of records \( (i, j) \) from two different datasets is a match (\( M_{ij} = 1 \)) or not (\( M_{ij} = 0 \)). Think of it as a sophisticated way of saying “these two records are talking about the same thing/person.”
- **Conditional Distributions**: Assumes conditional independence among linkage variables given the match status \( M_{ij} \). In other words, each variable's match status (like name, address) does not depend on each other after knowing whether the overall records match. This could enable the decomposition of the joint probability distribution into simpler, individual probabilities.
- **Handling Missing Data**: Utilizes a Missing At Random (MAR) framework to allow the omission of missing data in the probability calculations, which simplifies the likelihood function and enhances computational efficiency.

## Algorithm and Computation

### EM Algorithm
The parameter estimation is executed using the Expectation-Maximization (EM) algorithm. It starts with an initial guess, then repeatedly adjusts this guess aiming to improve the likelihood that the observed data came from the proposed model. This optimizes the observed-data likelihood function, which integrates over the probabilistic distributions of the linkage variables conditioned on the match hypotheses.

### Blocking and Filtering
To reduce computational demands:
- **Blocking**: To avoid comparing every record in one dataset with every record in another, which can be overwhelmingly time-consuming with large datasets, the model groups records into blocks based on shared characteristics (like all people with the same birth year), which greatly cuts down on unnecessary comparisons.
- **Filtering**: Eliminates highly unlikely pairs from consideration early in the process, using thresholds based on calculated probabilities.

### Scalability
The algorithm is designed to work efficiently even with very large datasets that contain millions of records. It uses parallel processing (splitting the work across multiple computer processors) and smart data structures to manage this, making it practical to run on a typical laptop without needing supercomputer resources.

## Evaluation and Implementation

### Simulation Studies
The model's robustness is tested through simulations that mimic real-world problems like incomplete data or errors in the data (measurement errors). These simulations help verify that the model can handle different types of common data issues effectively. The model is compared to traditional methods (like exact match), showing that it can handle complex, imperfect data more effectively and efficiently.

### Package Realization

Repository: https://github.com/kosukeimai/fastLink

Example: https://imai.fas.harvard.edu/research/files/turnout.pdf

## Package Implement

The required package for FastLink is called “fastLink”:

```{r, eval=FALSE}
install.packages("fastLink")
```

Install the most recent version of "fastLink" package (version 0.6):

```{r, eval=FALSE}
library(devtools)
install_github("kosukeimai/fastLink",dependencies=TRUE)
## Load the package and data
library(fastLink)
```

Tutorial Link: https://github.com/kosukeimai/fastLink

## Package Implement

Here we want to merge data frame A and data frame B:

```{r, eval=FALSE}
matches.out <- fastLink(
  dfA = dfA, dfB = dfB, 
  varnames = c("given_name", "surname", "address_1", "suburb"),
  stringdist.match = c("given_name", "surname"),
  partial.match = c("given_name", "surname"),
  return.all = TRUE
)
```

varnames: a vector containing the names of variables that will be used for matching, and these variable names must be present in both dfA and dfB.

stringdist.match: a vector of variable names selected from varnames. For the variables included in stringdist.match, agreement will be assessed using the Jaro-Winkler distance.

partial.match: a vector containing variable names that must be part of both stringdist.match and varnames. Variables listed in partial.match will have an additional partial agreement category calculated, alongside the disagreement and full agreement categories, based on the Jaro-Winkler distance.

The merged dataset can be accessed using the getMatches() function:

```{r, eval=FALSE}
matched_dfs <- getMatches(
  dfA = dfA, dfB = dfB, 
  fl.out = matches.out, threshold.match = 0.85
)
```

threshold.match: Lower bound for the posterior probability of a match that will be accepted. Default is 0.85.

Other functions available in the fastLink package:

1. Preprocessing Matches via Blocking: The blockData() function can block two datasets using one or more variables and various blocking techniques.

2. Using Auxiliary Information to Inform fastLink: The algorithm could also incorporate auxiliary information on migration behavior to inform the matching of datasets over time.

3. Aggregating Multiple Matches Together: The algorithm can also aggregate multiple matches into a single summary using the aggregateEM() function.

4. Random Sampling with fastLink: The algorithm allows us to run the matching algorithm on a randomly selected smaller subset of data to be matched and then apply those estimates to the full sample of data.

5. Finding Duplicates within a Dataset via fastLink: The algorithm uses the probabilistic match algorithm to identify duplicated entries.

## Implementations on Example Datasets

We implemented the FastLink method on two sets of datasets. The first set contains a dataset of products on Amazon with 1364 observations and another dataset of products on Google with 3227 observations.

## Statistical Analysis Post-Merging

### Uncertainty Quantification
The model quantifies the uncertainty in the merging process, allowing researchers to account for potential errors in subsequent analyses, which is critical for maintaining the integrity of research conclusions.

### Post-Merge Analysis
Discusses methodologies for incorporating the probabilities of matches into regression analyses and other statistical procedures to adjust for the uncertainty inherent in the linkage process.

## Contributions and Innovations

The model makes substantial contributions to the field of data management by providing:
- A robust probabilistic framework that substantially outperforms traditional deterministic methods. Unlike older methods that just said 'yes' or 'no' to whether records match, this model calculates how likely it is that records match. This approach gives us a clearer picture and usually results in better performance.
- Enhanced handling of missing data and the independence assumptions of linkage variables, which have been a significant limitation in earlier models.
- Detailed documentation and an accessible implementation in R, which facilitates reproducible research and widespread adoption in the social sciences.

```{r,warning=FALSE}
suppressMessages(require("fastLink"))
suppressMessages(require("plyr"))
data <- read.delim("cces2016voterval.tab")
summary(data)
```

## References

<div id="refs"></div>
